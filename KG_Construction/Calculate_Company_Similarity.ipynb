{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Company similarity\n",
    "\n",
    "#### compute similarity based on ['revenue', 'num_employees_enum', 'description','industries']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['name', 'headquaters_location_city', 'headquaters_location_state',\n",
       "       'headquaters_location_country', 'headquaters_regions', 'founder',\n",
       "       'website_link', 'hub', 'num_employees_enum', 'last_funding_type',\n",
       "       'rank_org_company', 'IPO_status', 'industries', 'stock_symbol',\n",
       "       'description', 'founded_date', 'operating_status', 'also_known_as',\n",
       "       'company_type', 'legal_name', 'contact_phone', 'number_of_exits',\n",
       "       'founded', 'type', 'revenue', 'website_link_prepro'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "company_df = pd.read_csv(\"company.csv\")\n",
    "company_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### # Apply hybrid similarity methods to get the most similar result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter useless attributes\n",
    "company_df = company_df[['name', 'num_employees_enum', 'industries', 'description', 'revenue']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deal with number of employees\n",
    "set(company_df['num_employees_enum'])\n",
    "# map the num_employees to level\n",
    "num_employees_dict = {'1 to 50 Employees': 1,\n",
    "                        '10000+ Employees': 5,\n",
    "                        '5001 to 10000 Employees': 4,\n",
    "                        '501 to 1000 Employees': 3,\n",
    "                        '51 to 200 Employees': 2\n",
    "                        }\n",
    "\n",
    "company_df['num_employees_enum'] = company_df['num_employees_enum'].apply(lambda x: num_employees_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_industries(string):\n",
    "    if type(string) == type(float('nan')):\n",
    "        return None\n",
    "    else:\n",
    "        return string.strip().split(',')\n",
    "\n",
    "company_df['industries'] = company_df['industries'].apply(lambda x: split_industries(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1753\n",
      "362\n"
     ]
    }
   ],
   "source": [
    "# look at the total number of industry name\n",
    "industry_names = []\n",
    "for names in list(company_df['industries']):\n",
    "    if names != None:\n",
    "        for name in names:\n",
    "            industry_names.append(name)\n",
    "print(len(industry_names))\n",
    "print(len(set(industry_names)))\n",
    "\n",
    "# then we should use jaccard similarity to compute the similarity based on industries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reveneue\n",
    "set(company_df['revenue'])\n",
    "revenue_dict = {'$1 to $5 billion (USD)': 5,\n",
    "                '$1 to $5 million (USD)': 1,\n",
    "                '$10+ billion (USD)': 7,\n",
    "                '$100 to $500 million (USD)': 3,\n",
    "                '$5 to $10 billion (USD)': 6,\n",
    "                '$5 to $25 million (USD)': 2,\n",
    "                '$500 million to $1 billion (USD)': 4,\n",
    "                'Unknown / Non-Applicable': None\n",
    "                }\n",
    "\n",
    "company_df['revenue'] = company_df['revenue'].apply(lambda x: revenue_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/august/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/august/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/august/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/august/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# description part can use tf-idf based cosine similarity to compute actuall similarity\n",
    "# transform description to arrays \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from strsimpy.normalized_levenshtein import NormalizedLevenshtein\n",
    "from strsimpy.jaro_winkler import JaroWinkler\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# preporcessing preparation\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stop_words.union(set(company_df['name']))\n",
    "jaro = JaroWinkler()\n",
    "# build punctuation dict\n",
    "punct_dict = dict((ord(punct), ' ') for punct in string.punctuation)\n",
    "lemmer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatizer(sentence):\n",
    "    # step 1: tokenize\n",
    "    word_tokens = word_tokenize(sentence.lower().translate(punct_dict))\n",
    "    # step 2: remove stopwords\n",
    "    filtered_tokens = [ token for token in word_tokens if token not in stop_words ]\n",
    "    # step 3: lemmatization\n",
    "    filtered_tokens = [lemmer.lemmatize(token) for token in filtered_tokens]\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/august/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le', 'u'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "documents = list(set(company_df['description'].dropna()))\n",
    "TfidfVec = TfidfVectorizer(tokenizer=lemmatizer, stop_words='english')\n",
    "\n",
    "def cos_similarity(textlist):\n",
    "    tfidf = TfidfVec.fit_transform(textlist)\n",
    "    return (tfidf * tfidf.T).toarray()\n",
    "\n",
    "similarity_res = cos_similarity(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# descrition_dict = dict()\n",
    "# for data in list(company_df['description']):\n",
    "#     descrition_dict[data] = descrition_dict.get(data, 0)+ 1\n",
    "# for key, value in descrition_dict.items():\n",
    "#     if value > 1:\n",
    "#         print(key, value)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overall_similarity(c1, c2):\n",
    "    # similariy over num_employees\n",
    "    if c1['num_employees_enum'] == None or c2['num_employees_enum'] == None:\n",
    "        num_employees_sim = 0\n",
    "    else:\n",
    "        if c1['num_employees_enum'] == c2['num_employees_enum']:\n",
    "            num_employees_sim = 1\n",
    "        else:\n",
    "            num_employees_sim = 1/abs(c1['num_employees_enum'] - c2['num_employees_enum'])\n",
    "    # similarity over revenue\n",
    "    if c1['revenue']==None or c2['revenue'] == None:\n",
    "        revenue_sim = 0\n",
    "    else:\n",
    "        if c1['revenue'] == c2['revenue']:\n",
    "            revenue_sim = 1\n",
    "        else:\n",
    "            revenue_sim = 1/abs(c1['revenue'] - c2['revenue'])\n",
    "    # similarity over industries with jaccard similarity\n",
    "    if c1['industries'] == None or c2['industries'] == None:\n",
    "        industry_sim = 0\n",
    "    else:\n",
    "        industry_sim = len(set(c1['industries']).intersection(set(c2['industries']))) / len(set(c1['industries']).union(set(c2['industries'])))\n",
    "    if c1['description']in [None,np.nan] or c2[\"description\"]in [None,np.nan]:\n",
    "        desc_sim = 0\n",
    "    else:\n",
    "        c1_desc_idx = documents.index(c1['description'])\n",
    "        c2_desc_idx = documents.index(c2[\"description\"])\n",
    "        desc_sim = similarity_res[c1_desc_idx][c2_desc_idx]\n",
    "\n",
    "    return 0.2 * num_employees_sim + 0.2*revenue_sim + 0.3*industry_sim + 0.3*desc_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_companies = []\n",
    "for i in range(len(company_df)):\n",
    "    sim_res = []\n",
    "    for j in [k for k in range(len(company_df)) if k != i]:\n",
    "        assert i != j\n",
    "        sim = overall_similarity(company_df.loc[i], company_df.loc[j])\n",
    "        sim_res.append((sim, j))\n",
    "    # sort data\n",
    "    sorted_res = sorted(sim_res, key=lambda x : x[0], reverse=True)\n",
    "    # print(sorted_res[:5])\n",
    "    similar_companies.append((company_df.loc[i]['name'], company_df.loc[sorted_res[0][1]][\"name\"], \\\n",
    "                                company_df.loc[sorted_res[1][1]][\"name\"], company_df.loc[sorted_res[2][1]][\"name\"], \\\n",
    "                                    company_df.loc[sorted_res[3][1]][\"name\"]))\n",
    "\n",
    "    # print('For company {}, the most three similar companies are {}, {}'.format(company_df.loc[i]['name'], company_df.loc[sim_res[0][1]][\"name\"],\n",
    "                                                                                       # company_df.loc[sim_res[1][1]][\"name\"]))                                                                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df = pd.DataFrame(similar_companies, columns=['name', 'sim_company_1', 'sim_company_2', 'sim_company_3', 'sim_company_4'])\n",
    "res_df.to_csv('similar_companies.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "files = []\n",
    "for filename in os.listdir('./glassdoor_reviews/'):\n",
    "    df = pd.read_csv('./glassdoor_reviews/' + filename)\n",
    "    files.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.concat(files).reset_index(drop=True)\n",
    "res = res.drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.to_csv('reviews.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a4d47218a8e022334151235867a278f0c38d0e926f731c3545fe924711bb8eee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
